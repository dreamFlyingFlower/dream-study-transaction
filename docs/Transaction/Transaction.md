# Transaction



# 分布式事务



* 单个程序访问不同的数据库或多个服务之间相互调用产生的事务一致性问题
* 若某个系统内部出现跨多个库的操作,是不合规的,应该每个服务只操作自己对应的数据库
* 若需要操作别的服务对应的库,不允许直连别的服务的库,违反微服务架构的规范
* 若随意交叉访问,服务是没法管理的,数据可能经常被别人改错,自己的库被别人写挂
* 若要操作其他服务的库,必须通过调用该服务的接口来实现,绝对不允许交叉访问别人的数据



# CAP



* CAP是Consistency,Availability,Partition tolerance的缩写,分别表示一致性,可用性,分区容忍性



## Consistency



* 一致性是指写操作后的读操作可以读取到最新的数据状态,当数据分布在多个节点上,从任意结点读取到的数据都是最新的状态
* 如何实现一致性:
  * 写入主数据库后要将数据同步到从数据库
  * 写入主数据库后,在向从数据库同步期间要将从数据库锁定,待同步完成后再释放锁,以免在新数据写入成功后,向从数据库查询到旧的数据
* 分布式系统一致性的特点:
  * 由于存在数据同步的过程,写操作的响应会有一定的延迟
  * 为了保证数据一致性会对资源暂时锁定,待数据同步完成释放锁定资源
  * 如果请求数据同步失败的结点则会返回错误信息,一定不会返回旧数据



## Availability



* 可用性是指任何事务操作都可以得到响应结果,且不会出现响应超时或响应错误
* 如何实现可用性:
  * 写入主数据库后要将数据同步到从数据库
  * 由于要保证从数据库的可用性,不可将从数据库中的资源进行锁定
  * 即时数据还没有同步过来,从数据库也要返回要查询的数据,哪怕是旧数据,如果连旧数据也没有则可以按照约定返回一个默认信息,但不能返回错误或响应超时
* 分布式系统可用性的特点:
  * 所有请求都有响应,且不会出现响应超时或响应错误



## Partition tolerance



* 通常分布式系统的各各结点部署在不同的子网,这就是网络分区,不可避免的会出现由于网络问题而导致结点之间通信失败,此时仍可对外提供服务,这叫分区容忍性
* 如何实现分区容忍性:
  * 尽量使用异步取代同步操作,例如使用异步方式将数据从主数据库同步到从数据,这样结点之间能有效的实现松耦
  * 添加从数据库结点,其中一个从结点挂掉其它从结点提供服务
* 分布式分区容忍性的特点:
  * 分区容忍性分是布式系统具备的基本能力



# BASE



## 强一致性和最终一致性



* CAP理论说明一个分布式系统最多只能同时满足一致性,可用性和分区容忍性中的两项
* AP在实际应用中较多,AP即舍弃一致性,保证可用性和分区容忍性,但是在实际生产中很多场景都要实现一致性,即使不要一致性,但是最终也要将数据同步成功来保证数据一致.这种一致性和CAP的一致性不同,CAP中的一致性要求在任何时间查询每个结点数据都必须一致,是强一致性,但是最终一致性允许在一段时间内每个结点的数据不一致,但是经过一段时间每个结点的数据必须一致,强调的是最终数据的一致性
* BASE 是 Basically Available(基本可用),Soft state(软状态)和 Eventually consistent(最终一致性)的缩写.BASE理论是对CAP中AP的一个扩展,通过牺牲强一致性来获得可用性,当出现故障允许部分不可用但要保证核心功能可用,允许数据在一段时间内是不一致的,但最终达到一致状态.满足BASE理论的事务称之柔性事务



## 基本可用



* 分布式系统在出现故障时,允许损失部分可用功能,保证核心功能可用.如,电商网站交易付款出现问题了,商品依然可以正常浏览



## 软状态



* 由于不要求强一致性,所以BASE允许系统中存在中间状态(也叫软状态),这个状态不影响系统可用性,如订单的支付中,数据同步中等状态,待数据最终一致后状态改为成功状态



## 最终一致



* 最终一致是指经过一段时间后,所有节点数据都将会达到一致.如订单的支付中状态,最终会变为支付成功或者支付失败,使订单状态与实际交易结果达成一致,但需要一定时间的延迟,等待



# 分布式唯一ID



## 概述



* 全局唯一性,不可出现重复id数据
* 需要防止恶意用户根据id规则来获取数据
* 保证下一个id一定大于上一个id



## UUID



* 优点:代码实现简单,不占用宽带,数据迁移不受影响
* 缺点:无序,无法保证递增趋势,字符存储,传输以及查询慢,不可读



## SnowFlake



* 雪花算法,实现方式是用64位的二进制分别表示不同的含义:
  * 从左到右的第1位为0,固定
  * 第2位到42位,总共41位,表示二进制的毫秒时间戳
  * 第43位到47位,总共5位,表示二进制的机房编号
  * 第48位到52位,总共5位,表示二进制的机器编号
  * 第53位到64位,总共12位,表示二进制的数据编号,顺序生成
* 优点:代码实现简单,不占用宽带,数据迁移不受影响,低位趋势递增
* 缺点:强依赖时钟(多台服务器时间一定要一样),无序无法保证趋势递增



## Redis



* 集群缩减版本:年份+当前这天属于这一年的第多少天+小时+redis自增:2+3+2+6=13位.调用intr方法

* 优点:不依赖数据,灵活方便,性能优于数据库自增长,没有单点故障
* 缺点:需要占用网络资源,性能要比本地生成慢,需要增加redis集群



## Zookeeper



* 集群环境下生成方式同Redis,调用的是InterProcessMutex方法
* 优缺点同Redis差不多



# 分布式锁



## Redis



* nx方法,若key存在则返回nil,若key不存在则设置值
* 需要使用到特定的脚本才行,详见官网的set方法



## Redisson



* Redis封装的专门用于分布式的各种锁,信号量等机制的框架



## Zookeeper



* 临时节点的新增删除,集群中同一时间只能有一个节点进行新增



# 2PC-两阶段提交



![](TX01.PNG)



## 核心



* 事务管理器(TM),管理器负责协调多个数据库的事务
* 准备阶段(Prepare phase):TM给每个参与者发送Prepare消息,每个数据库参与者在本地执行事务,并写本地的Undo/Redo日志,此时事务没有提交
* 提交阶段(Commit phase):如果TM收到了参与者的执行失败或者超时消息时,直接给每个参与者发送回滚消息;否则,发送提交(Commit)消息.参与者根据TM的指令执行提交或者回滚操作,并释放事务处理过程中使用的锁资源
* 必须在最后阶段释放锁资源



## 缺陷



* 单点故障:TM出错,网络问题,事务提交可能会失败
* 如果第二阶段的TM挂了,那么事务A,B就不知道该如何处理事务了
* 如果A事务收到了TM的消息,执行了事务,但是B没收到,此时B事务该如何处理,事务不一致
* 阻塞资源:占用数据库连接.如果网络出现问题,会一直占用数据库连接,性能低
* 数据不一致:二阶段回滚出错或部分事务没有收到TM发送的消息,会造成数据不一致
* 因为严重依赖于数据库层面来搞定复杂的事务,效率很低,不适合高并发的场景



## 适用



* 比较适合单块应用里,跨多个库的分布式事务
* 基于Spring + JTA的组合可以实现该事务



# 3PC-三阶段提交



![](TX02.PNG)



* 三阶段提交比二阶段提交多了个准备提交阶段,会将一阶段的结果告诉各个事务.如果三阶段TM挂了或网络超时,各本地事务仍然可以根据二阶段的结果做相应的事务操作
* 但这也会带来数据一致性的问题,常见的做法有:增加异步的数据补偿任务,日终跑批前的数据补偿,更完善的业务数据完整性的校验代码,引入数据监控及时通知人工补录这些都是不错的补救措施



# XA



 * Transaction Manager(TM):事务管理器,控制全局事务的边界,负责开启一个全局事务,并最终发起全局提交或全局回滚的决议
 * Resource Manager(RM):资源管理器,事务的参与者,一般情况下是指一个数据库实例,通过资源管理器对该数据库进行控制,资源管理器控制着分支事务
* 内部XA:单机情况下, 由binlog充当TM的角色.一个事务过来,写入redo.log和undo.log.事务提交时,同时写入redo.log和bin.log,保证redo.log和bin.log一致,如果事务撤销,则根据undo.log进行撤销
* 外部XA:分布式集群的情况下,一般加代理层来充当TM的角色,实现对事务的支持



# TCC



 ![](TCC00.png)



* TCC的全称是:Try->Confirm->Cancel
  * Try:对各个服务的资源做检测,对资源进行锁定或者预留
  * Confirm:在各个服务中执行实际的操作,即使用Try阶段资源
  * Cancel:若任何一个服务出错,就进行补偿,将已经执行成功的业务逻辑回滚,释放Try预留的资源
* 例如跨银行银行转账,若用TCC方案解决如下
  * Try:先把两个银行账户中的资金给它冻结住就不让操作了
  * Confirm:执行实际的转账操作,A银行账户的资金扣减,B银行账户的资金增加
  * Cancel:若任何一个银行的操作失败,就需要回滚进行补偿.比如A银行账户如果已经扣减了,但是B银行账户资金增加失败了,那么就得把A银行账户资金给加回去
* 这种方案几乎很少使用,因为这个事务回滚严重依赖于自己写代码来回滚和补偿,会造成补偿代码巨大
* 一般来说跟钱,交易相关的场景可以用TCC,严格保证分布式事务,要么全部成功,要么全部自动回滚



## 幂等控制



* 使用TCC时要进行幂等控制,网络或重试都有可能导致这几个操作的重复执行

* TCC需要在的二阶段Try,Confirm和Cancel接口保证幂等,这样不会重复使用或者释放资源

* 如银行转账

  * A向B转账1000,在confirm()中,A余额-1000,冻结余额-1000,这一步是实现幂等性的关键
  * 在操作资金账户时,为了防止并发情况下数据不一致的出现,肯定会避免出现这种代码

  ```java
  //根据userId查到账户
  Account account = accountMapper.selectById(userId);
  //取出当前资金
  int availableMoney = account.getAvailableMoney();
  account.setAvailableMoney(availableMoney-1000);
  //更新剩余资金
  accountMapper.update(account);
  ```

  * 上述操作本质上是一个读-改-写的过程,不是原子的,在并发情况下会出现数据不一致问题
  * 所以最简单的做法是直接update

  ```sql
  update account set available_money = available_money-1000 where user_id=#{userId}
  ```

  * 这利用了数据库行锁特性解决了并发下的数据不一致问题,但是TCC中,单纯使用这个方法不行
  * 该方法能解决并发单次操作下的扣减余额问题,但是不能解决多次操作带来的多次扣减问题,假设执行了两次,按这种方案,用户账户就少了2000块
  
  

### 解决方案



* 解决思路在上述分支事务记录中增加执行状态,每次执行前都查询该状态  

* 可以引入转账订单状态来做判断,若订单状态为已支付,则直接return
* 新建一张去重表,用订单id做唯一键,若插入报错返回也可以



## 空回滚



* 事务协调器在调用TCC服务的一阶段Try操作时,可能会出现因为丢包而导致的网络超时,此时事务协调器会触发二阶段回滚,调用TCC服务的Cancel操作
* TCC服务在未收到Try请求的情况下收到Cancel请求,这种场景被称为空回滚
* TCC服务在实现Cancel时应当允许空回滚的执行

![](TCC01.png)



### 解决方案



* 第一种:如果try()没执行,那么订单一定没创建,所以cancle()里可以加一个判断,如果上下文中订单编号orderNo不存在或者订单不存在,直接返回成功
* 该方案的核心思想就是回滚请求处理时,如果对应的具体业务数据为空,则返回成功

* 第二种:通过中间件或数据库来实现
* TM在发起全局事务时生成全局事务记录,全局事务ID贯穿整个分布式事务调用链条.再额外增加一张分支事务记录表,其中有全局事务ID和分支事务ID.第一阶段 Try 方法里会插入一条记录,表示一阶段执行了.Cancel 接口里读取该记录,如果该记录存在,则正常回滚;如果该记录不存在,则是空回滚  



## 防悬挂



![](TCC02.png)



* 事务协调器在调用TCC服务的一阶段Try操作时,可能会因网络而导致超时,此时事务协调器会触发二阶段回滚,调用Cancel操作.在此之后,拥堵在网络上的一阶段Try数据包被TCC服务收到,出现了二阶段Cancel请求比一阶段Try请求先执行的情况
* 用户在实现TCC服务时,应当允许空回滚,但是要拒绝执行空回滚之后到来的一阶段Try请求



### 解决方案



* 可以在二阶段执行时插入一条事务控制记录,状态为已回滚,这样当一阶段执行时,先读取该记录,如果记录存在,就认为二阶段回滚操作已经执行,不再执行try方法



## Tcc-transaction



* [github](https://github.com/changmingxie/tcc-transaction)地址
* 和Seata类似,需要部署一个独立的服务才能运行



## Hmily



* [github](https://github.com/dromara/hmily)
* 不需要独立部署
* 支持嵌套事务
* 采用disruptor框架进行事务日志的异步读写,与RPC框架的性能毫无差别
* 支持SpringBoot-starter 项目启动,使用简单
* RPC框架支持:dubbo,motan,springcloud
* 本地事务存储支持:redis,mongodb,zookeeper,file,mysql
* 事务日志序列化支持:java,hessian,kryo,protostuff
* 采用Aspect AOP切面思想与Spring无缝集成,天然支持集群
* RPC事务恢复,超时异常恢复等



### 原理



* Hmily利用AOP对参与分布式事务的本地方法与远程方法进行拦截处理,通过多方拦截,事务参与者能透明的调用到另一方的Try,Confirm,Cancel方法,传递事务上下文,并记录事务日志,酌情进行补偿,重试等
* Hmily不需要事务协调服务,但需要提供一个数据库(mysql/mongodb/zookeeper/redis/file)来进行日志存储
* Hmily实现TCC服务与普通的服务一样,只需要暴露一个接口,也就是Try,Confirm/Cancel业务逻辑,只是因为全局事务提交/回滚的需要才提供的,因此Confirm/Cancel业务只需要被Hmily TCC事务框架发现即可,不需要被调用它的其他业务服务所感知



# 本地消息表



* 消息队列+本地事件表+定时任务

* A系统在自己本地一个事务里操作,同时插入一条数据到消息表,接着A系统将这个消息发送到MQ中去
* B系统接收到消息之后,在一个事务里,往自己本地消息表里插入一条数据,同时执行其他的业务操作,如果这个消息已经被处理过了,那么此时这个事务会回滚,这样保证不会重复处理消息
* B系统执行成功之后,就会更新自己本地消息表的状态以及A系统消息表的状态
* 如果B系统处理失败了,那么就不会更新消息表状态,那么此时A系统会定时扫描自己的消息表,如果有没处理的消息,会再次发送到MQ中去,让B再次处理
* 这个方案保证了最终一致性,哪怕B事务失败了,但是A会不断重发消息,直到B成功为止
* 这个方案最大的问题就在于严重依赖于数据库的消息表来管理事务.完全无法胜任高并发场景,也不好扩展



# MQ消息最终一致性



![](tx03-01.png)

 

* 这里的MQ专指RocketMQ,不用本地的消息表,直接基于RocketMQ来实现事务
* A发送事务消息至MQ,MQ将消息状态标记为Prepared,此时该消息B无法消费,如果Prepared消息发送失败那么就直接取消操作不执行
* MQ收到A发送的消息并持久化之后回应A,表示MQ已收到消息
* A收到MQ的回执确认后执行本地事务
  * 若A本地事务执行成功则向MQ发送Commit,MQ收到Commit后将状态标记为可消费,此时B即可正常消费消息
  * 若A本地事务执行失败则向MQ发送Rollback,MQ收到Rollback消息后将删除消息
* B消费消息,消费成功则向MQ回应Ack,否则将重复接收消息,这里Ack默认自动回应
* 事务回查:如果A挂掉或超时,MQ收不到Commit或Rollback,则要进行事务回查(超时询问)
  * A除了实现正常的业务流程外,还需提供一个事务询问的接口,供MQ调用
  * 当消息中间件收到发布消息便开始计时,如果到了超时没收到确认指令,就会主动调用A提供的事务询问接口询问该系统目前的状态,该接口会返回三种结果,MQ根据三种结果做出不同反应:
    * 提交:将该消息投递给系统B
    * 回滚:直接将条消息丢弃
    * 处理中:继续等待  

* MQ会根据事务回查结果来决定是否投递消息
* MQ向B投递完消息后进入阻塞等待状态,B系统便立即进行任务的处理,处理完便向MQ发送Ack
* B服务的事务失败的解决方案:
  * 重试,自动不断重试直到成功,最好设置重试次数,尽量不影响业务的正常运行
  * 实在无法成功,事务回滚.但是一般是人工处理B事务使其成功,因为AB都回滚比较浪费资源
* 对于事务不是特别严谨的场景比较合适,但是会存在资源浪费



## 独立消息服务



* 消息服务子系统
* 消息管理子系统
* 消息状态确认子系统
* 消息恢复子系统
* 实时消息服务



# MQ最大努力通知



![](TX04.png)



* 系统A处理业务的同一事务中,向本地消息表中写入一条记录
* 准备专门的消息发送者不断地发送本地消息表中的消息到MQ,如果发送失败则重试
* MQ收到消息后负责将该消息同步投递给相应的B,并触发B的任务执行  
* 若系统B执行成功就事务完成
* 若系统B执行失败,就定时尝试重新调用系统B,反复N次,最后还是不行就放弃,同时写入错误消息表
* MQ需要提供失败消息的查询接口,B会定期查询失败消息,并将其消费
* 这里面会存在一个最大努力通知系统,提供A,B中查询本地消息的接口
* 一个充值的例子

![](tx04-01.png)





## 方案一



![](TX05.png)

* 利用MQ的ack机制由MQ向接收通知方发送通知,流程如下
* 发起通知方将通知发给MQ
* 使用普通消息机制将通知发给MQ
* 如果消息没有发出去可由接收通知方主动请求发起通知方查询业务执行结果
* 接收通知方监听 MQ
* 接收通知方接收消息,业务处理完成回应ack
* 接收通知方若没有回应ack则MQ会重复通知
* MQ会按照间隔1min,5min,10min,30min,1h,2h,5h,10h的方式,逐步拉大通知间隔,直到达到通知要求的时间窗口上限
* 接收通知方可通过消息校对接口来校对消息的一致性



## 方案二



![](TX06.PNG)

* 利用MQ的ack机制,与上面不同的是应用程序向接收通知方发送通知
  * 发起通知方将通知发给MQ.使用可靠消息一致方案中的事务消息保证本地事务与消息的原子性,最终将通知先发给MQ
  * 通知程序监听 MQ,接收MQ的消息.上面的方案接收通知方直接监听MQ,本方案由通知程序监听MQ.通知程序若没有回应ack则MQ会重复通知
  * 通知程序通过互联网接口协议调用接收通知方案接口,完成通知.通知程序调用接收通知方案接口成功就表示通知成功,即消费MQ消息成功,MQ将不再向通知程序投递通知消息
  * 接收通知方可通过消息校对接口来校对消息的一致性



## 区别



* 方案1中接收通知方与MQ接口,即接收通知方案监听 MQ,此方案主要应用与内部应用之间的通知
* 方案2中由通知程序与MQ接口,通知程序监听MQ,收到MQ的消息后由通知程序通过互联网接口协议调用接收通知方.此方案主要应用于外部应用之间的通知,例如支付宝,微信的支付结果通知



# 对比



## 2PC



* 最大的诟病是一个阻塞协议.RM在执行分支事务后需要等待TM的决定,此时服务会阻塞并锁定资源.由于其阻塞机制和最差时间复杂度高, 因此,这种设计不能适应随着事务涉及的服务数量增加而扩展的需要,很难用于并发较高以及子事务生命周期较长 (long-running transactions) 的分布式服务中



## TCC



* 如果拿TCC事务的处理流程与2PC两阶段提交做比较,2PC通常都是在跨库的DB层面,而TCC则在应用层面的处理,需要通过业务逻辑来实现
* 这种分布式事务的实现方式的优势在于,可以让应用自己定义数据操作的粒度,使得降低锁冲突、提高吞吐量成为可能.不足之处则在于对应用的侵入性非常强,业务逻辑的每个分支都需要实现try,confirm,cancel三个操作.此外,其实现难度也比较大,需要按照网络状态、系统故障等不同的失败原因实现不同的回滚策略.典型的使用场景：满,登录送优惠券等



## 可靠消息最终一致性



* 事务适合执行周期长且实时性要求不高的场景.引入消息机制后,同步的事务操作变为基于消息执行的异步操作, 避免了分布式事务中的同步阻塞操作的影响,并实现了两个服务的解耦.典型的使用场景:注册送积分,登录送优惠券等



## 最大努力通知



* 是分布式事务中要求最低的一种,适用于一些最终一致性时间敏感度低的业务；允许发起通知方处理业务失败,在接收通知方收到通知后积极进行失败处理,无论发起通知方如何处理结果都会不影响到接收通知方的后续处理;发起通知方需提供查询执行情况接口,用于接收通知方校对结果.典型的使用场景:银行通知、支付结果通知等
